\section{Buffering Strategies for Large Matrices}
\label{sec:strategies}


Whenever the matrices are too large for all the allocations to fit on GPU, then they are split into blocks.
The algorithm can choose two values:
\begin{itemize}
	\item \code{num\_split\_common\_dim}, the number of splits for A's rows and B's columns. When splitting in this direction, a single value of C will be the sum of this amount of products among blocks of A and B.
	\item\code{num\_split\_other\_dim}, the number of splits for A's columns and B's rows. This will result in C being made of a square grid of blocks, with this value as edge. The different blocks will be independent from each other.
\end{itemize}

An example of matrix splitting is provided in figure~\ref{img:example-of-splits}.

\img{example-of-splits}{.6\linewidth}{An example of how matrices A and B can be split}

In order to choose how to split, the algorithm computes the required memory, and compares it with the available memory.
If the available memory is smaller by a factor $k$, then:
\begin{itemize}
	\itemsep 0em
	\item \code{num\_split\_common\_dim} is assigned the value $\sqrt{k}$.
	\item\code{num\_split\_other\_dim} is assigned the value $\lceil \frac{k}{num\_split\_other\_dim} \rceil$.
\end{itemize}

\subsection{Strategy 1: no buffering}

This is the naive approach: the GPU memory is divided into 3 blocks: A, B and C.
At every product, the iteration loads A and B in parallel, then sums the product into C.
Whenever all the multiplications for a block of C are done, the block itself is unloaded to the CPU memory, while in the meantime the next A and B can start loading.

A timing diagram of this strategy can be seen in figure~\ref{}.

\subsection{Strategy 2: double buffer for A and B}

When using strategy 1, the multiplication requires A and B to be loaded: this means that while A and B are being transfered to GPU, no computation is being done (except for the checksums, that are however very fast).
Similarly, loading the next A and B requires the product to be completed, thus having no memory transfer while computing the product.
Effectively, this means that the 3 CUDA queues are mostly disjoint: the only overlaps are H2D and D2H when copying C (not at every iteration), and H2D and compute while calculating the checksums (for a very short time).

Strategy 2 aims to improve this situation, by dividing the GPU memory in two extra blocks (5 in total, called A, A', B, B' and C).
Initially, A and B are pre-loaded.
Then, before starting the product, A' and B' start to load the next block, in an asynchronous way.
At the same time, C can compute the product on A and B.
When both the new loading and the product are finished, the buffers are swapped: C can immediately start multiplying A' and B', while A and B load the next block.
This allows to overlap the H2D and compute queues in the time when the ``offline'' A and B are loading.

Figure~\ref{} shows the timing relative to this strategy.

\subsection{Strategy 3: double buffer for A, B and C}

This strategy brings to the extreme the idea behind strategy 2.
In strategy 2, multiplications are still forbidden while C is offloading, to avoid overriding not-yet-saved results.
Strategy 3 introduces a new buffer, called C', that works similarly to A' and B'.
At the beginning, the product is computed on C.
Then, when a full block has been computed, C is switched with C', to be able to immediately start the computation, while the offloading of C can go on in the background.
This ideally allows the compute stream (the bottleneck of the complex task of matrix multiplications) to run continuously, regardless of the status of the transfer queues.

Differently from strategy 2, however, the gained parallelism only occurs once every \texttt{num\_split\_common\_dim} iterations: this fact makes the speedup less impactful, with the risk of it being shadowed by the reduction in block sizes required to fit C' in the GPU memory, that can lead to more multiplications required.

Figure~\ref{} displays the timing diagram, in the optimal case when the addition of C' does not require more multiplications.

\subsection{Strategy 4: double concurrent product}

Since the slowest part of the program is the actual computation of the product, we tried to execute more than one in parallel.
Under the (uncertain) assumption that two multiplications can be done in parallel, this would really speed up the program.
Moreover, if the assumption is false, this idea should not be slower than strategy 3.

Strategy 4 uses the buffers C and C' to compute a block of the final matrix into C, and the block to its right into C', if it exists.
Using this method, the two products multiply the same block of A with shifted blocks of C: therefore, we can use just 3 buffers for the operands: A, B and B'.
This increases the size of each buffer, and reduces the required memory copies.

For this final strategy we draw the timing diagram for the two cases: if the parallel multiplications hypothesis holds (figure~\ref{}) or if it is wrong (figure~\ref{}).

\subsection{Strassen algorithm}

We initially discussed about adding an option to use Strassen algorithm at this higher level.
Ideally, it could be applied with any of the strategies described above.
The problem of this algorithm is that it requires 7 temporary matrices, for which we saw two options: storing all them in GPU memory, or loading and unloading them at need.

In order to compare the two options among them and with the traditional algorithm, we considered the case described in figure~\ref{img:matmul-2x2-normal}, where A and B do not fit on GPU memory, but they fit if they are divided into 4 blocks.

\img{matmul-2x2-normal}{.5\linewidth}{A product of 2x2 matrices with the traditional algorithm}
When using Strassen algorithm, the product would be as described in figure~\ref{img:matmul-2x2-strassen}.

As visible in figure~\ref{img:matmul-flow-normal}, the traditional algorithm requires 20 memory transfers, to load the blocks of A and B, and to offload C when computed.
This example assumes that the GPU is divided into 3 blocks of equal size (the 3 squares on top of each other, in the image)
\img{matmul-flow-normal}{\linewidth}{The series of load/calc/unload operations for a 2x2 product with the traditional algorithm}
If we want to use Strassen algorithm with blocks of the same size, figure~\ref{img:matmul-flow-strassen-more-transfers} reveals that 45 memory transfers are required, because the program needs to offload temporary sums and matrices to make space for the other values.
\img{matmul-flow-strassen-more-transfers}{\linewidth}{The series of load/calc/unload operations for a 2x2 product with Strassen algorithm, if we want to have the GPU memory divided in just 3 blocks}
If instead we decide to have more blocks for the Strassen algorithm, in order to avoid offloading temporary results, we would need 21 memory transfers, as shown in figure~\ref{img:matmul-flow-strassen-more-memory}.
This removes one multiplication with respect to the traditional algorithm, at the cost of adding just an extra memory transfer.
In principle it would be good, if not for the fact that this approach requires the blocks to have half the size, thus having on average $\sqrt{2}$ more blocks.
That would then require more multiplications than what the Strassen algorithm saves, thus making its application not beneficial.
\img{matmul-flow-strassen-more-memory}{\linewidth}{The series of load/calc/unload operations for a 2x2 product with Strassen algorithm, if we want to avoid offloading temporary results}

As a conclusion, we realized that Strassen algorithm was an efficient way to save computation time, but only if the full matrices were fully available on GPU, thus making memory transfers not needed.
