By priority:

- use batching of single float transfers whenever possible
- clean code, check documentation/comments
- async copy while computing buffering schemes:
    - AB->C (default)
    - AB->C // A'B' (double buffering)
    - AB->C // A'B'->C' (parallel computation)
- profile everything
- can further optimize block sizes, memory... ?

Paper:

- problem statement
- consideration on which algo to use (we considered strassen but [show calculations of transactions])
- our orchestration of the kernels and buffers
- the safe mm operation (how do we use the checksum, how do we correct/detect errors)
- results (profiling) 

If there is time:

- strassen matrix multiplication

There will be no time:

- use tensor cores (cublas) when available
https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/
- split into gpus
